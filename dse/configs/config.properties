# ------------------- #
# Cassandra Props
# ------------------- #
cassandra.config=/home/cassandra_debezium.yaml
cassandra.hosts=127.0.0.1
cassandra.port=9042
# cassandra.username=cassandra
# cassandra.password=cassandra
# cassandra.ssl.enabled=false
# cassandra.ssl.config.path=/

# ------------------- #
# Kafka Props
# ------------------- #
kafka.producer.bootstrap.servers=kafka:9092
kafka.producer.retries=3
kafka.producer.retry.backoff.ms=1000
kafka.topic.prefix=cassandra
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

# ------------------- #
# Debezium Props
# ------------------- #
connector.name=source_connect_1
snapshot.consistency=ONE
# Specifies the criteria for running a snapshot (eg. initial sync) upon startup of the cassandra connector agent. Must be one of 'INITIAL', 'ALWAYS', or 'NEVER'. The default snapshot mode is 'INITIAL'.
snapshot.mode=NEVER
# The port used by the HTTP server for ping, health check, and build info
http.port=8000
# Determines whether or not the CommitLogPostProcessor should run to move processed commit logs from relocation dir. If disabled, commit logs would not be moved out of relocation dir.
commit.log.post.processing.enabled=true
# The class used by CommitLogPostProcessor to move processed commit logs from relocation dir. The built-in transfer class is BlackHoleCommitLogTransfer,which simply removes all processed commit logs from relocation dir. Users are supposed to implement their own customized commit log transfer class if needed.
commit.log.transfer.class=io.debezium.connector.cassandra.BlackHoleCommitLogTransfer
# The local directory where commit logs get relocated to from cdc_raw dir once processed.
commit.log.relocation.dir=/opt/dse/resources/debezium/relocation
commit.log.relocation.dir.poll.interval.ms=10000
# Determines whether or not the CommitLogProcessor should re-process error commit logs.
commit.log.error.reprocessing.enabled=false
offset.backing.store.dir=/opt/dse/resources/debezium/offsets
# The minimum amount of time to wait before committing the offset. The default value of 0 implies the offset will be flushed every time.
offset.flush.interval.ms=0
# The maximum records that are allowed to be processed until it is required to flush offset to disk. This config is effective only if offset_flush_interval_ms != 0.
max.offset.flush.size=100
# Positive integer value that specifies the maximum size of the blocking queue into which change events read from the commit log are placed before they are written to Kafka. This queue can provide back pressure to the commit log reader when, for example, writes to Kafka are slower or if Kafka is not available. Events that appear in the queue are not included in the offsets periodically recorded by this connector. Defaults to 8192, and should always be larger than the maximum batch size specifiedin the max.batch.size property. The capacity of the queue to hold deserialized records before they are converted to Kafka Connect structs and emitted to Kafka.
max.queue.size=8192
# The maximum number of change events to dequeue each time.
max.batch.size=2048
# Long value for the maximum size in bytes of the blocking queue. The feature is disabled by default, it will be active if it’s set with a positive long value.
max.queue.size.in.bytes=0
# Positive integer value that specifies the number of milliseconds the commit log processor should wait during each iteration for new change events to appear in the queue. Defaults to 1000 milliseconds, or 1 second.
poll.interval.ms=10000
# Positive integer value that specifies the number of milliseconds the schema processor should wait before refreshing the cached Cassandra table schemas.
schema.refresh.interval.ms=5000
# The maximum amount of time to wait on each poll before reattempt.
cdc.dir.poll.interval.ms=10000
# Positive integer value that specifies the number of milliseconds the snapshot processor should wait before re-scanning tables to look for new cdc-enabled tables. Defaults to 10000 milliseconds, or 10 seconds.
snapshot.scan.interval.ms=5000
# Whether deletion events should have a subsequent tombstone event (true) or not (false). It’s important to note that in Cassandra, two events with the same key may be updating different columns of a given table. So this could potentially result in records being lost during compaction if they have not been consumed by the consumer yet. In other words, do NOT set this to true if you have Kafka compaction turned on.
tombstones.on.delete=false
# A comma-separated list of fully-qualified names of fields that should be excluded from change event message values. Fully-qualified names for fields are in the form <keyspace_name>.<field_name>.<nested_field_name>.
# field.exclude.list=testdb.customers.removed_field
# The number of change event queues and queue processors. Defaults to 1.
num.of.change.event.queues=1
# true when connector configuration explicitly specifies the key.converter or value.converter parameters to use Avro, otherwise defaults to false. Whether field names will be sanitized to adhere to Avro naming requirements. See Avro naming for more details.
sanitize.field.names=false
# comma-separated list of operation types that will be skipped during streaming. The operations include: c for inserts/createu for updatesd for deletes. By default, no operations are skipped.
# skipped.operations=d